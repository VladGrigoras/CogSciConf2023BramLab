---
title: "EIG Zendo"
author: "VladG"
date: '2022-12-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)

selfreport <- read.csv("selfreport.csv")
tests <- read.csv("tests.csv")
#Experiment mentions that "rocks of an exact colour OR shape must be present for radiation"
#18 possible rules (hypothesis space):
  # 1 - 1 red
  # 2 - 2 red
  # 3 - 3 red
  # 4 - 1 purple
  # 5 - 2 purple
  # 6 - 3 purple
  # 7 - 1 green
  # 8 - 2 green
  # 9 - 3 green
  # 10 - 1 triangle
  # 11 - 2 triangles
  # 12 - 3 triangles
  # 13 - 1 stick
  # 14 - 2 sticks
  # 15 - 3 sticks
  # 16 - 1 square
  # 17 - 2 squares
  # 18 - 3 squares
```

```{r}
#change the encodings to make our life easier when working

tests$size[tests$size == 1] <- 13
tests$size[tests$size == 2] <- 10
tests$size[tests$size == 3] <- 16
tests$colour[tests$colour == "red"] <- 1
tests$colour[tests$colour == "blue"] <- 4
tests$colour[tests$colour == "green"] <- 7

tests$colour <- as.numeric(tests$colour)

#Encoding now is:
#1 - red
#4 - purple
#7 - green
#10 - triangle
#13 - stick
#16 - square
```

```{r}
#Essentially all features are number coded as the rule space above and this function helps us extract the scene from the multiple test rows

#Rember that each feature is coded as mentioned previously and I am jumping around with regards to object number as in the hypothesis encoding

#By feature I mean a certain colour or a certain shape

extract = function(feature,state){
  #If we have already observed an object with this feature, then move to encode the presence of 2 objects with this feature
  if(state[feature] == 1){ 
    state[feature] <- 0
    state[feature + 1] <- 1 
  } 
  
  #Otherwise, if this is the first time we are observing this feature, we encode the presence of 1 object with this feature
  
  else if((state[feature] == 0) & (state[feature + 1] == 0)) state[feature] <- 1
  
  #Otherwise, it must be that it is the third time we observe this feature, so we encode the presence of 3 objects with this feature
  
  else{
    state[feature + 2] <- 1
    state[feature + 1] <- 0 
  }
  state  
}
```

```{r}
#generate the scene state akin to switch_state in the other paper

j <- 1 #tracks row in tests
state <- rep(0, 18)

for(i in 1:nrow(selfreport)){
  
  #recreate the scene from each individual object in a test 
  
  repeat{
    state <- extract(tests$size[j],state)
    state <- extract(tests$colour[j],state)
    j <- j + 1
    
    if(!(tests$test[j] == tests$test[j-1]) | j > nrow(tests)){ break}
  }
  
  #this recreates the scene with respect to the hypotheses space
  
  selfreport$s1[i] <- state[1]
  selfreport$s2[i] <- state[2]
  selfreport$s3[i] <- state[3]
  selfreport$s4[i] <- state[4]
  selfreport$s5[i] <- state[5]
  selfreport$s6[i] <- state[6]
  selfreport$s7[i] <- state[7]
  selfreport$s8[i] <- state[8]
  selfreport$s9[i] <- state[9]
  selfreport$s10[i] <- state[10]
  selfreport$s11[i] <- state[11]
  selfreport$s12[i] <- state[12]
  selfreport$s13[i] <- state[13]
  selfreport$s14[i] <- state[14]
  selfreport$s15[i] <- state[15]
  selfreport$s16[i] <- state[16]
  selfreport$s17[i] <- state[17]
  selfreport$s18[i] <- state[18]
  
  selfreport$outcome[i] <- tests$follow_rule[j-1] #add the outcome to each individual scene
  
  state <- rep(0, 18)
}

#turn the outcome from boolean to int
selfreport$outcome[selfreport$outcome == TRUE] <- 1
selfreport$outcome[selfreport$outcome == FALSE] <- 0

```

prior is $$P(H)$$

prior_entropy is $$SE(H)$$

```{r}
#This is essentially identical to the sparse implementation in "compute_entropy.R" for the "Children's failure to control variables may reflect adaptive decision making" paper by Jones et al., 2020

for (i in 1:nrow(selfreport))
{
  zendo_state = c(selfreport$s1[i], selfreport$s2[i], selfreport$s3[i], selfreport$s4[i], selfreport$s5[i], selfreport$s6[i], selfreport$s7[i], selfreport$s8[i], selfreport$s9[i], selfreport$s10[i], selfreport$s11[i], selfreport$s12[i], selfreport$s13[i], selfreport$s14[i], selfreport$s15[i], selfreport$s16[i], selfreport$s17[i], selfreport$s18[i])

  if (selfreport$test[i]==1){
    #On the first test its uniform
    prior<-rep(1/18, 18)
    prior_entropy<- -18 *(1/18*log2(1/18))
  } 
  else {
    #Subsequently the prior is the previous test's posterior
    prior<-c(selfreport$p1[i-1], selfreport$p2[i-1], selfreport$p3[i-1], selfreport$p4[i-1], selfreport$p5[i-1], selfreport$p6[i-1], selfreport$p7[i-1], selfreport$p8[i-1], selfreport$p9[i-1], selfreport$p10[i-1], selfreport$p11[i-1], selfreport$p12[i-1], selfreport$p13[i-1], selfreport$p14[i-1], selfreport$p15[i-1], selfreport$p16[i-1], selfreport$p17[i-1], selfreport$p18[i-1])
    prior_entropy<-selfreport$entropy[i-1]
  }
  
  post.un<-post_alt.un<-prior
  post.un[zendo_state!=selfreport$outcome[i]]<-0#Unnormalised probability of each rule after the outcome (positive or negative outcome)
  post_alt.un[zendo_state==selfreport$outcome[i]]<-0#The probability of the alternative thing happening
  
  if (selfreport$outcome[i]==1){
    marg<-sum(prior[zendo_state==1], na.rm = T)#The likelihood of the actual outcome  
  } else {
    marg<-sum(prior[zendo_state==0], na.rm = T)#The likelihood of the actual outcome
  }  
  
  post<-post.un/sum(post.un, na.rm = T)
  post_alt<-post_alt.un/sum(post_alt.un, na.rm = T)
  
  selfreport$prior1[i]<-prior[1]
  selfreport$prior2[i]<-prior[2]
  selfreport$prior3[i]<-prior[3]
  selfreport$prior4[i]<-prior[4]
  selfreport$prior5[i]<-prior[5]
  selfreport$prior6[i]<-prior[6]
  selfreport$prior7[i]<-prior[7]
  selfreport$prior8[i]<-prior[8]
  selfreport$prior9[i]<-prior[9]
  selfreport$prior10[i]<-prior[10]
  selfreport$prior11[i]<-prior[11]
  selfreport$prior12[i]<-prior[12]
  selfreport$prior13[i]<-prior[13]
  selfreport$prior14[i]<-prior[14]
  selfreport$prior15[i]<-prior[15]
  selfreport$prior16[i]<-prior[16]
  selfreport$prior17[i]<-prior[17]
  selfreport$prior18[i]<-prior[18]
  
  selfreport$p1[i]<-post[1]
  selfreport$p2[i]<-post[2]
  selfreport$p3[i]<-post[3]
  selfreport$p4[i]<-post[4]
  selfreport$p5[i]<-post[5]
  selfreport$p6[i]<-post[6]
  selfreport$p7[i]<-post[7]
  selfreport$p8[i]<-post[8]
  selfreport$p9[i]<-post[9]
  selfreport$p10[i]<-post[10]
  selfreport$p11[i]<-post[11]
  selfreport$p12[i]<-post[12]
  selfreport$p13[i]<-post[13]
  selfreport$p14[i]<-post[14]
  selfreport$p15[i]<-post[15]
  selfreport$p16[i]<-post[16]
  selfreport$p17[i]<-post[17]
  selfreport$p18[i]<-post[18]
  
  selfreport$prior_entropy[i]<-prior_entropy
  
  selfreport$entropy[i] = -sum(post * log2(post), na.rm=T) #SE(H | s, o)
  
  selfreport$information[i] = prior_entropy - selfreport$entropy[i]
  
  ent_alt<- -sum(post_alt * log2(post_alt), na.rm=T)
  
  selfreport$eig1[i] <- prior_entropy - marg*(selfreport$entropy[i]) - (1-marg) * ent_alt
  
  #
  n_left<-sum(prior!=0)
  if (n_left%in%c(6,4,2))
  {
    selfreport$eigmax[i]<-1
  } else if (n_left==5)
  {
    selfreport$eigmax[i]<-(-log2(1/5) - -log2(1/2))*2/5 +  (-log2(1/5) - -log2(1/3))*3/5#.97
  } else if (n_left==3)
  {
    selfreport$eigmax[i]<-(-log2(1/3) - 0)*1/3 +  (-log2(1/3) - -log2(1/2))*2/3#.91
  } else {
    selfreport$eigmax[i]<-0
  }
  selfreport$guess_p[i] = post[selfreport$guess[i]]
  selfreport$max_p[i]<-max(post)
  selfreport$positive_outcome_p[i] = marg[1]
  
}
```
